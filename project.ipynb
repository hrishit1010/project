{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Installing  Lib\n",
        "!pip uninstall duckduckgo-search -y\n",
        "!pip install ddgs\n",
        "!pip install beautifulsoup4\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hHoRpx8ZTMv",
        "outputId": "11a2767a-f9dd-4f5d-edd6-57f5ad5fc9e1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping duckduckgo-search as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ddgs\n",
            "  Downloading ddgs-9.10.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.1)\n",
            "Collecting primp>=0.15.0 (from ddgs)\n",
            "  Downloading primp-1.0.0-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: lxml>=4.9.4 in /usr/local/lib/python3.12/dist-packages (from ddgs) (6.0.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Collecting fake-useragent>=2.2.0 (from ddgs)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.2.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
            "Downloading ddgs-9.10.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading primp-1.0.0-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: socksio, primp, fake-useragent, ddgs\n",
            "Successfully installed ddgs-9.10.0 fake-useragent-2.2.0 primp-1.0.0 socksio-1.0.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from ddgs import DDGS\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import os\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "EXCEL_FILE = \"financial_data.xlsx\""
      ],
      "metadata": {
        "id": "hVeJ_EtEZYSH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parsing query to extarct period and other details\n",
        "def parse_user_query(query):\n",
        "    kpis = [\"Stock\", \"Revenue\", \"EBITDA\", \"Profit\", \"earnings per share\"]\n",
        "    periods = [\"jan\", \"Feb\", \"Q3\", \"Q4\", \"FY\", \"2026\", \"2024\", \"2025\"]\n",
        "\n",
        "    found_kpi = next((k for k in kpis if k.lower() in query.lower()), None)\n",
        "    found_period = next((p for p in periods if p.lower() in query.lower()), None)\n",
        "\n",
        "    company = query.split()[0]\n",
        "\n",
        "    return {\n",
        "        \"company\": company,\n",
        "        \"kpi\": found_kpi,\n",
        "        \"period\": found_period\n",
        "    }"
      ],
      "metadata": {
        "id": "rtDIT4LiZb5Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it will use duck go and search 5 url based on query\n",
        "from ddgs import DDGS\n",
        "\n",
        "def search_web(query, max_results=5):\n",
        "    urls = []\n",
        "    with DDGS() as ddgs:\n",
        "        for r in ddgs.text(query):\n",
        "            urls.append(r[\"href\"])\n",
        "            if len(urls) >= max_results:\n",
        "                break\n",
        "    return urls"
      ],
      "metadata": {
        "id": "h2g4SxvSZegD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scraps 5  webpage and extracts raw readable text.\n",
        "def scrape_page(url):\n",
        "    try:\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0\"\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        text = soup.get_text(separator=\" \", strip=True)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"Scrape error:\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "8EmP24xgZhO7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the raw data from webpage into excel or other formats based on user requirements\n",
        "def store_document_excel(company, kpi, period, value, url):\n",
        "\n",
        "    new_data = {\n",
        "        \"company\": company,\n",
        "        \"kpi\": kpi,\n",
        "        \"period\": period,\n",
        "        \"value\": value,\n",
        "        \"url\": url,\n",
        "        \"timestamp\": datetime.now()\n",
        "    }\n",
        "\n",
        "    if os.path.exists(EXCEL_FILE):\n",
        "        df = pd.read_excel(EXCEL_FILE)\n",
        "\n",
        "        # Deduplicate by URL + KPI + period\n",
        "        duplicate = df[\n",
        "            (df[\"url\"] == url) &\n",
        "            (df[\"kpi\"] == kpi) &\n",
        "            (df[\"period\"] == period)\n",
        "        ]\n",
        "\n",
        "        if not duplicate.empty:\n",
        "            print(\"Duplicate skipped\")\n",
        "            return\n",
        "\n",
        "        df = pd.concat([df, pd.DataFrame([new_data])], ignore_index=True)\n",
        "    else:\n",
        "        df = pd.DataFrame([new_data])\n",
        "\n",
        "    df.to_excel(EXCEL_FILE, index=False)\n",
        "    print(\"Stored:\", url)"
      ],
      "metadata": {
        "id": "sB70Z8s8ZjOd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applied regex to extract kpi like ebitda,revenue earned per share\n",
        "def extract_financial_value(content, kpi):\n",
        "    # This regex looks for the KPI,\n",
        "    # then an optional currency symbol, optional space,\n",
        "    # then the numeric part (allowing for thousands separators and decimals),\n",
        "    # and finally an optional scale word (billion/million/B/M/T), followed by a word boundary.\n",
        "    pattern = rf\"{kpi}.*?(\\$|€|₹|£)?\\s*(\\d{{1,3}}(?:[.,]\\d{{3}})*(?:[.,]\\d+)?\\s*(?:billion|million|B|M|T)?)\\b\"\n",
        "    matches = re.findall(pattern, content, re.IGNORECASE)\n",
        "\n",
        "    if matches:\n",
        "        # Group 1: currency symbol (e.g., '$')\n",
        "        # Group 2: the numeric part including its scale (e.g., '100,000.5 billion')\n",
        "        currency, value_with_scale = matches[0]\n",
        "        return f\"{currency}{value_with_scale.strip()}\"\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "PB-GYxbvZlKt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connecting everything user query,parsing,finding websites for scrapping\n",
        "def financial_agent(user_query):\n",
        "\n",
        "    print(\"User Query:\", user_query)\n",
        "\n",
        "    parsed = parse_user_query(user_query)\n",
        "    print(\"Parsed:\", parsed)\n",
        "\n",
        "    if not parsed[\"kpi\"]:\n",
        "        print(\"KPI not detected in query.\")\n",
        "        return\n",
        "\n",
        "    search_query = f\"{parsed['company']} {parsed['kpi']} {parsed['period']} financial results\"\n",
        "    print(\"Search Query:\", search_query)\n",
        "\n",
        "    urls = search_web(search_query)\n",
        "    print(\"URLs Found:\", len(urls))\n",
        "\n",
        "    structured_results = []\n",
        "\n",
        "    for url in urls:\n",
        "        print(\"Scraping:\", url)\n",
        "\n",
        "        content = scrape_page(url)\n",
        "        if not content:\n",
        "            continue\n",
        "\n",
        "        value = extract_financial_value(content, parsed[\"kpi\"])\n",
        "\n",
        "        if value:\n",
        "            store_document_excel(\n",
        "                parsed[\"company\"],\n",
        "                parsed[\"kpi\"],\n",
        "                parsed[\"period\"],\n",
        "                value,\n",
        "                url\n",
        "            )\n",
        "\n",
        "            structured_results.append({\n",
        "                \"company\": parsed[\"company\"],\n",
        "                \"kpi\": parsed[\"kpi\"],\n",
        "                \"period\": parsed[\"period\"],\n",
        "                \"value\": value,\n",
        "                \"source\": url\n",
        "            })\n",
        "\n",
        "    return structured_results"
      ],
      "metadata": {
        "id": "3ucMVm6hZt9H"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#type any prompt\n",
        "results = financial_agent(\"apple revenue feb 2025\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3DHd0hnZv03",
        "outputId": "4bc26212-123b-46b0-dd17-b1e95bcea785"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User Query: apple revenue feb 2025\n",
            "Parsed: {'company': 'apple', 'kpi': 'Revenue', 'period': 'Feb'}\n",
            "Search Query: apple Revenue Feb financial results\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:primp.impersonate:Impersonate 'chrome_107' does not exist, using 'random'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URLs Found: 5\n",
            "Scraping: https://www.apple.com/newsroom/2026/01/apple-reports-first-quarter-results/\n",
            "Stored: https://www.apple.com/newsroom/2026/01/apple-reports-first-quarter-results/\n",
            "Scraping: https://investor.apple.com/investor-relations/default.aspx\n",
            "Scraping: https://www.reuters.com/business/apple-sales-profit-beat-wall-street-estimates-amid-staggering-iphone-demand-2026-01-29/\n",
            "Scraping: https://www.macrotrends.net/stocks/charts/AAPL/apple/revenue\n",
            "Scraping: https://finance.yahoo.com/quote/AAPL/financials/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'company': 'apple',\n",
              "  'kpi': 'Revenue',\n",
              "  'period': 'Feb',\n",
              "  'value': '026',\n",
              "  'source': 'https://www.apple.com/newsroom/2026/01/apple-reports-first-quarter-results/'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(EXCEL_FILE):\n",
        "    print(f\"Downloading {EXCEL_FILE}...\")\n",
        "    files.download(EXCEL_FILE)\n",
        "else:\n",
        "    print(\"Excel file not found. Please run the financial_agent() function to generate it.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "_qmV59zRa7CT",
        "outputId": "ae058b07-d8cf-4ce8-ed2f-e08b1bbf0fad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading financial_data.xlsx...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aef51e34-caf9-46fa-83a4-60730b3701e8\", \"financial_data.xlsx\", 5142)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}